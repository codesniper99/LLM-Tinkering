{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20739b3c-6b77-4845-81f7-face8ca34192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "ORCA_KEY = os.getenv(\"ORCA_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97fa3919-e66b-48d0-bc04-6b188db6d11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z4/WVglMdry+vBmRyGcFOMnEfg52U005P3a8gh+oskSHsNbqxO/HxA==\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import gradio as gr\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pvorca\n",
    "import soundfile as sf\n",
    "import spaces\n",
    "import torch\n",
    "import xxhash\n",
    "from datasets import Audio\n",
    "from transformers.models.auto.modeling_auto import AutoModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "print(ORCA_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3a3438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ORCA_KEY\n",
    "orca = pvorca.create(\n",
    "    access_key=ORCA_KEY,\n",
    "    model_path=\"./static/orca_params_en_male.pv\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3180e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type  to instantiate a model of type diva. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "LOADER_STR = \"Loading.....\"\n",
    "if gr.NO_RELOAD:\n",
    "    diva_model = AutoModel.from_pretrained(\n",
    "        \"WillHeld/DiVA-llama-3.2-1b\", trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    resampler = Audio(sampling_rate=16_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6333d663-10f0-4a91-8694-6e0361a11f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type  to instantiate a model of type diva. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "@spaces.GPU\n",
    "@torch.no_grad\n",
    "def diva_audio(audio_input, do_sample=False, temperature=0.001, prev_outs=None):\n",
    "    sr, y = audio_input\n",
    "    x = xxhash.xxh32(bytes(y)).hexdigest()\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "    a = resampler.decode_example(\n",
    "        resampler.encode_example({\"array\": y, \"sampling_rate\": sr})\n",
    "    )\n",
    "    yield from diva_model.generate_stream(\n",
    "        a[\"array\"],\n",
    "        (\n",
    "            \"Your name is DiVA, which stands for Distilled Voice Assistant. You were trained with early-fusion training to merge OpenAI's Whisper and Meta AI's Llama 3 8B to provide end-to-end voice processing. You should respond in a conversational style. The user is talking to you with their voice and you are responding with text. Use fewer than 20 words.\"\n",
    "            if prev_outs == None\n",
    "            else None\n",
    "        ),\n",
    "        do_sample=do_sample,\n",
    "        max_new_tokens=256,\n",
    "        init_outputs=prev_outs,\n",
    "        return_outputs=True,\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AppState:\n",
    "    conversation: list = field(default_factory=list)\n",
    "    stopped: bool = False\n",
    "    model_outs: any = None\n",
    "\n",
    "\n",
    "def process_audio(audio: tuple, state: AppState):\n",
    "    return audio, state\n",
    "\n",
    "\n",
    "@spaces.GPU(duration=40, progress=gr.Progress(track_tqdm=True))\n",
    "def response(state: AppState, audio: tuple):\n",
    "    if not audio:\n",
    "        return AppState()\n",
    "\n",
    "    file_name = f\"/tmp/{xxhash.xxh32(bytes(audio[1])).hexdigest()}.wav\"\n",
    "\n",
    "    sf.write(file_name, audio[1], audio[0], format=\"wav\")\n",
    "\n",
    "    state.conversation.append(\n",
    "        {\"role\": \"user\", \"content\": {\"path\": file_name, \"mime_type\": \"audio/wav\"}}\n",
    "    )\n",
    "    if state.model_outs is None:\n",
    "        gr.Warning(\n",
    "            \"The first response might take a second to generate as DiVA is loaded from Disk to the ZeroGPU!\"\n",
    "        )\n",
    "    state.conversation.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": LOADER_STR,\n",
    "        }\n",
    "    )\n",
    "    yield state, state.conversation, None\n",
    "    if spaces.config.Config.zero_gpu:\n",
    "        if state.model_outs is not None:\n",
    "            state.model_outs = tuple(\n",
    "                tuple(torch.tensor(vec).cuda() for vec in tup)\n",
    "                for tup in state.model_outs\n",
    "            )\n",
    "        causal_outs = (\n",
    "            CausalLMOutputWithPast(past_key_values=state.model_outs)\n",
    "            if state.model_outs\n",
    "            else None\n",
    "        )\n",
    "    else:\n",
    "        causal_outs = state.model_outs\n",
    "    state.model_outs = None\n",
    "    prev_outs = causal_outs\n",
    "    stream = orca.stream_open()\n",
    "\n",
    "    buff = []\n",
    "    for resp, outs in diva_audio(\n",
    "        (audio[0], audio[1]),\n",
    "        prev_outs=(prev_outs if prev_outs is not None else None),\n",
    "    ):\n",
    "        prev_resp = state.conversation[-1][\"content\"]\n",
    "        if prev_resp == LOADER_STR:\n",
    "            prev_resp = \"\"\n",
    "        state.conversation[-1][\"content\"] = resp\n",
    "        pcm = stream.synthesize(resp[len(prev_resp) :])\n",
    "        audio_chunk = None\n",
    "        if pcm is not None:\n",
    "            buff.extend(pcm)\n",
    "        if len(buff) > (orca.sample_rate * 2):\n",
    "            mp3_io = io.BytesIO()\n",
    "            sf.write(\n",
    "                mp3_io,\n",
    "                np.asarray(buff[: orca.sample_rate]).astype(np.int16),\n",
    "                orca.sample_rate,\n",
    "                format=\"mp3\",\n",
    "            )\n",
    "            audio_chunk = mp3_io.getvalue()\n",
    "            mp3_io.close()\n",
    "            buff = buff[orca.sample_rate :]\n",
    "        yield state, state.conversation, audio_chunk\n",
    "\n",
    "    del outs.logits\n",
    "    del outs.hidden_states\n",
    "    if spaces.config.Config.zero_gpu:\n",
    "        outs = tuple(\n",
    "            tuple(vec.cpu().numpy() for vec in tup) for tup in outs.past_key_values\n",
    "        )\n",
    "    audio_chunk = None\n",
    "    pcm = stream.flush()\n",
    "    if pcm is not None:\n",
    "        mp3_io = io.BytesIO()\n",
    "        sf.write(\n",
    "            mp3_io,\n",
    "            np.asarray(buff + pcm).astype(np.int16),\n",
    "            orca.sample_rate,\n",
    "            format=\"mp3\",\n",
    "        )\n",
    "        audio_chunk = mp3_io.getvalue()\n",
    "        mp3_io.close()\n",
    "    stream.close()\n",
    "    yield (\n",
    "        AppState(conversation=state.conversation, model_outs=outs),\n",
    "        state.conversation,\n",
    "        audio_chunk,\n",
    "    )\n",
    "\n",
    "\n",
    "def start_recording_user(state: AppState):\n",
    "    return None\n",
    "\n",
    "\n",
    "theme = gr.themes.Soft(\n",
    "    primary_hue=gr.themes.Color(\n",
    "        c100=\"#82000019\",\n",
    "        c200=\"#82000033\",\n",
    "        c300=\"#8200004c\",\n",
    "        c400=\"#82000066\",\n",
    "        c50=\"#8200007f\",\n",
    "        c500=\"#8200007f\",\n",
    "        c600=\"#82000099\",\n",
    "        c700=\"#820000b2\",\n",
    "        c800=\"#820000cc\",\n",
    "        c900=\"#820000e5\",\n",
    "        c950=\"#820000f2\",\n",
    "    ),\n",
    "    secondary_hue=\"rose\",\n",
    "    neutral_hue=\"stone\",\n",
    ")\n",
    "\n",
    "js = \"\"\"\n",
    "async function main() {\n",
    "  const script1 = document.createElement(\"script\");\n",
    "  script1.src = \"https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.js\";\n",
    "  document.head.appendChild(script1)\n",
    "  const script2 = document.createElement(\"script\");\n",
    "  script2.onload = async () =>  {\n",
    "    console.log(\"vad loaded\") ;\n",
    "    var record = document.querySelector('.record-button');\n",
    "    record.textContent = \"Just Start Talking!\"\n",
    "    record.style = \"width: fit-content; padding-right: 0.5vw;\"\n",
    "    const myvad = await vad.MicVAD.new({\n",
    "      onSpeechStart: () => {\n",
    "        var record = document.querySelector('.record-button');\n",
    "        var player = document.getElementById(\"streaming_out\").querySelector(\".standard-player\")\n",
    "        if (record != null && (player == null || player.paused)) {\n",
    "          console.log(record);\n",
    "          record.click();\n",
    "        }\n",
    "      },\n",
    "      onSpeechEnd: (audio) => {\n",
    "        var stop = document.querySelector('.stop-button');\n",
    "        if (stop != null) {\n",
    "          console.log(stop);\n",
    "          stop.click();\n",
    "        }\n",
    "      }\n",
    "    })\n",
    "    myvad.start()\n",
    "  }\n",
    "  script2.src = \"https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.min.js\";\n",
    "  script1.onload = () =>  {\n",
    "    console.log(\"onnx loaded\") \n",
    "    document.head.appendChild(script2)\n",
    "  };\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "js_reset = \"\"\"\n",
    "() => {\n",
    "  var record = document.querySelector('.record-button');\n",
    "  record.textContent = \"Just Start Talking!\"\n",
    "  record.style = \"width: fit-content; padding-right: 0.5vw;\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(theme=theme, js=js) as demo:\n",
    "    with gr.Row():\n",
    "        input_audio = gr.Audio(\n",
    "            label=\"Input Audio\",\n",
    "            sources=[\"microphone\"],\n",
    "            type=\"numpy\",\n",
    "            streaming=False,\n",
    "            waveform_options=gr.WaveformOptions(waveform_color=\"#B83A4B\"),\n",
    "        )\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(label=\"Conversation\", type=\"messages\")\n",
    "    with gr.Row(max_height=\"50vh\"):\n",
    "        output_audio = gr.Audio(\n",
    "            label=\"Output Audio\",\n",
    "            streaming=True,\n",
    "            autoplay=True,\n",
    "            visible=True,\n",
    "            elem_id=\"streaming_out\",\n",
    "        )\n",
    "    state = gr.State(value=AppState())\n",
    "    stream = input_audio.start_recording(\n",
    "        process_audio,\n",
    "        [input_audio, state],\n",
    "        [input_audio, state],\n",
    "    )\n",
    "    respond = input_audio.stop_recording(\n",
    "        response, [state, input_audio], [state, chatbot, output_audio]\n",
    "    )\n",
    "    restart = respond.then(start_recording_user, [state], [input_audio]).then(\n",
    "        lambda state: state, state, state, js=js_reset\n",
    "    )\n",
    "\n",
    "    cancel = gr.Button(\"Restart Conversation\", variant=\"stop\")\n",
    "    cancel.click(\n",
    "        lambda: (AppState(), gr.Audio(recording=False)),\n",
    "        None,\n",
    "        [state, input_audio],\n",
    "        cancels=[respond, restart],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a5d72-953b-4c9a-9fe4-6e2708c95463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-with-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
